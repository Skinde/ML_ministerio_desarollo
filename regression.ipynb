{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "496f4bc6",
   "metadata": {},
   "source": [
    "# **Ministry Budget: Regression**\n",
    "\n",
    "A partir de nuestros datos limpios vamos a proceder con realizar una regresión no lineal. Elegimos esta pues en nuestra análisis y exploración de datos notamos que no existe correlación entre variables, pues el coeficiente es muy bajo, esto quiere decir que no existe constante que mantenga la proporción, algo común considerando que mediana y promedio difieren."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63bfd46d",
   "metadata": {},
   "source": [
    "---\n",
    "## **Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f65ef4b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np;\n",
    "import pandas as pd;\n",
    "import matplotlib.pyplot as plt;\n",
    "\n",
    "from sklearn.model_selection import train_test_split;\n",
    "from sklearn.preprocessing import StandardScaler;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f8fb64",
   "metadata": {},
   "source": [
    "---\n",
    "## **No-linear Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42b01c6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NonlinearRegression:\n",
    "    def __init__(self,  poly_degree, X):\n",
    "        self.poly_degree = poly_degree;\n",
    "        xshape1,xshape2 = X.shape\n",
    "        self.weights = np.random.rand(xshape1,xshape2);\n",
    "        self.bias = np.random.random();\n",
    "\n",
    "    def poly_features(self, X):\n",
    "        n_samples, n_features = X.shape;\n",
    "        features_vector = np.ones((n_samples, 1));\n",
    "\n",
    "        for degree in range(1, self.poly_degree + 1):\n",
    "            for feature in range(n_features):\n",
    "                new_feature = X[:, feature] ** degree;\n",
    "                features_vector = np.hstack((features_vector, new_feature.reshape(n_samples, 1)));\n",
    "        \n",
    "        self.weights = np.random.rand(features_vector.shape[1])\n",
    "        return features_vector;\n",
    "\n",
    "    def predict(self, X):\n",
    "        X_poly = self.poly_features(X);\n",
    "        return np.dot(X_poly, self.weights) + self.bias;\n",
    "\n",
    "    def loss_function(self, X, Y, poly_lambda, reg_type='L2'):\n",
    "        y_pred = self.predict(X);\n",
    "        base_loss = np.mean((Y - y_pred) ** 2) / 2;\n",
    "\n",
    "        if reg_type == 'L1':\n",
    "            regularization = poly_lambda * np.sum(np.abs(self.weights));\n",
    "        else:\n",
    "            regularization = poly_lambda * np.sum(self.weights ** 2);\n",
    "\n",
    "        return base_loss + regularization;\n",
    "\n",
    "    def gradients(self, X, Y, poly_lambda):\n",
    "        n = len(Y)\n",
    "        X_poly = self.poly_features(X)\n",
    "        y_pred = self.predict(X)\n",
    "        error = Y - y_pred\n",
    "        dw = -(np.dot(X_poly.T, error) / n) + 2 * poly_lambda * self.weights\n",
    "        db = -np.sum(error) / n\n",
    "        return dw, db\n",
    "\n",
    "    def update_params(self, dw, db, poly_alpha):\n",
    "        self.weights -= poly_alpha * dw\n",
    "        self.bias -= poly_alpha * db\n",
    "\n",
    "    def train(self, X, Y, poly_alpha, poly_epochs, poly_lambda):\n",
    "        for epoch in range(poly_epochs):\n",
    "            dw, db = self.gradients(X, Y, poly_lambda)\n",
    "            self.update_params(dw, db, poly_alpha)\n",
    "\n",
    "            if epoch % 1000 == 0:\n",
    "                loss = self.loss_function(X, Y, poly_lambda)\n",
    "                print(f'Epoch {epoch}, Loss: {loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d17b3f",
   "metadata": {},
   "source": [
    "Ahora en este punto ya contamos con el modelo no lineal, lo que nos quedaría realizar es la parte del entrenamiento y testeo de datos, de tal forma que podamos iterar hasta llegar al mínimo error. Para ello continuamos de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa37be94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1464 entries, 0 to 1463\n",
      "Data columns (total 11 columns):\n",
      " #   Column           Non-Null Count  Dtype\n",
      "---  ------           --------------  -----\n",
      " 0   PROGRAMA_PPTAL   1464 non-null   int64\n",
      " 1   ACT_OBRA_ACCINV  1464 non-null   int64\n",
      " 2   GRUPO_FN         1464 non-null   int64\n",
      " 3   META             1464 non-null   int64\n",
      " 4   AVAN_FISICO_SEM  1464 non-null   int64\n",
      " 5   DISTRITO         1464 non-null   int64\n",
      " 6   GENERICA         1464 non-null   int64\n",
      " 7   SUBGENERICA      1464 non-null   int64\n",
      " 8   SUBGENERICA_DET  1464 non-null   int64\n",
      " 9   ESPECIFICA       1464 non-null   int64\n",
      " 10  ESPECIFICA_DET   1464 non-null   int64\n",
      "dtypes: int64(11)\n",
      "memory usage: 125.9 KB\n",
      "None\n",
      "   PROGRAMA_PPTAL  ACT_OBRA_ACCINV  GRUPO_FN  META  AVAN_FISICO_SEM  DISTRITO  \\\n",
      "0               1                2         2     6                6        17   \n",
      "1               1                2         2    20                6        17   \n",
      "2               2                6         5     4               16        17   \n",
      "3               1                2         2     1                6        13   \n",
      "4               1                2         2     7                6        17   \n",
      "\n",
      "   GENERICA  SUBGENERICA  SUBGENERICA_DET  ESPECIFICA  ESPECIFICA_DET  \n",
      "0         0            1               17          32              71  \n",
      "1         0            1                4          27              45  \n",
      "2         0            1               20           4              27  \n",
      "3         0            1               17          32              39  \n",
      "4         0            1               19          18              73  \n"
     ]
    }
   ],
   "source": [
    "np.random.seed(11);\n",
    "\n",
    "train_data = pd.read_csv('./db/trainDataModified.csv');\n",
    "\n",
    "x_data = train_data.drop(['MTO_PIA'], axis=1);\n",
    "y_data = train_data['MTO_PIA'];\n",
    "\n",
    "print(x_data.info());\n",
    "print(x_data.head(5));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296c42c2",
   "metadata": {},
   "source": [
    "Una vez que hemos cargado nuestra data y hemos apartado la columna MTO_PIA, es hora de crear nuestra data de test y de entrenamiento, para ello tomamos un 20% para entrenamiento. Por otro lado, vamos a usar normalizar nuestros datos utilizando la función StandardScaler de la librería de Sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "232058be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      PROGRAMA_PPTAL  ACT_OBRA_ACCINV  GRUPO_FN  META  AVAN_FISICO_SEM  \\\n",
      "240                1                0         0     1                6   \n",
      "1138               1                0         0     1                6   \n",
      "1394               1                0         0     1                6   \n",
      "286                1                0         0     1                6   \n",
      "1282               1                0         0     1                6   \n",
      "...              ...              ...       ...   ...              ...   \n",
      "533                2                6         5    26                0   \n",
      "976                2                5         5    17                0   \n",
      "638                2                6         5    12               51   \n",
      "1341               2                6         5    24                4   \n",
      "1104               2                6         5    25                4   \n",
      "\n",
      "      DISTRITO  GENERICA  SUBGENERICA  SUBGENERICA_DET  ESPECIFICA  \\\n",
      "240         13         0            1               20           4   \n",
      "1138        13         0            1               19          36   \n",
      "1394        13         0            1                4          27   \n",
      "286         13         0            1               20           4   \n",
      "1282        13         0            1               19          36   \n",
      "...        ...       ...          ...              ...         ...   \n",
      "533         17         0            1               19          18   \n",
      "976         15         1            2                8          15   \n",
      "638         17         0            1               19          18   \n",
      "1341        17         0            0               11           2   \n",
      "1104        17         0            0               16          10   \n",
      "\n",
      "      ESPECIFICA_DET  \n",
      "240                3  \n",
      "1138              38  \n",
      "1394              20  \n",
      "286               27  \n",
      "1282              28  \n",
      "...              ...  \n",
      "533               73  \n",
      "976               65  \n",
      "638               44  \n",
      "1341              33  \n",
      "1104              55  \n",
      "\n",
      "[1171 rows x 11 columns]\n",
      "[[ 0.8784891   0.86170101  0.87074498 ... -1.24624575  0.6892336\n",
      "   0.80555427]\n",
      " [-1.11513636 -1.13050409 -1.10914312 ... -1.24624575 -0.3899891\n",
      "   0.26652338]\n",
      " [ 0.8784891   0.86170101  0.87074498 ...  0.92964587 -1.05412615\n",
      "  -0.31742674]\n",
      " ...\n",
      " [ 0.8784891   0.36364974  0.87074498 ... -1.07886947 -0.14093771\n",
      "   1.38950439]\n",
      " [-1.11513636 -1.13050409 -1.10914312 ...  0.09276448  1.10431926\n",
      "  -1.48532699]\n",
      " [ 0.8784891   0.86170101  0.87074498 ...  0.26014076 -0.55602336\n",
      "   0.94031199]]\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=11)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train);\n",
    "x_test_scaled = scaler.transform(x_test);\n",
    "\n",
    "print(x_train.sort_values(by='GRUPO_FN'));\n",
    "print(x_train_scaled);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2500cf",
   "metadata": {},
   "source": [
    "Ahora es momento de poner a prueba nuestro modelo, para ello tenemos un diccionario con los parámetros que se le asignarán a nuestro modelo, fácil de actualizar. No obstante invocamos nuestro modelo de entrenamiento y además buscamos las variables predictoras tanto para train como para test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c51ee40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 18689520272.68854\n",
      "Epoch 100, Loss: 18489716420.31615\n",
      "Epoch 200, Loss: 18326152950.674778\n",
      "Epoch 300, Loss: 18192187988.640736\n",
      "Epoch 400, Loss: 18082630101.93538\n",
      "Epoch 500, Loss: 17992890730.74262\n",
      "Epoch 600, Loss: 17919365503.808666\n",
      "Epoch 700, Loss: 17859250174.817715\n",
      "Epoch 800, Loss: 17809991533.0637\n",
      "Epoch 900, Loss: 17769718408.446003\n",
      "Epoch 1000, Loss: 17736734159.862057\n",
      "Epoch 1100, Loss: 17709701014.79629\n",
      "Epoch 1200, Loss: 17687574105.607677\n",
      "Epoch 1300, Loss: 17669485572.166264\n",
      "Epoch 1400, Loss: 17654637778.90388\n",
      "Epoch 1500, Loss: 17642539145.79811\n",
      "Epoch 1600, Loss: 17632592744.282295\n",
      "Epoch 1700, Loss: 17624507446.527233\n",
      "Epoch 1800, Loss: 17617776649.40861\n",
      "Epoch 1900, Loss: 17612371515.11124\n",
      "Epoch 2000, Loss: 17607884327.52615\n",
      "Epoch 2100, Loss: 17604231566.40974\n",
      "Epoch 2200, Loss: 17601270511.149265\n",
      "Epoch 2300, Loss: 17598771874.84303\n",
      "Epoch 2400, Loss: 17596807224.07543\n",
      "Epoch 2500, Loss: 17595173518.03724\n",
      "Epoch 2600, Loss: 17593834507.453026\n",
      "Epoch 2700, Loss: 17592711587.735252\n",
      "Epoch 2800, Loss: 17591796139.903667\n",
      "Epoch 2900, Loss: 17591059327.91126\n",
      "Epoch 3000, Loss: 17590489506.57891\n",
      "Epoch 3100, Loss: 17589972941.787334\n",
      "Epoch 3200, Loss: 17589587059.018997\n",
      "Epoch 3300, Loss: 17589196113.17884\n",
      "Epoch 3400, Loss: 17588983356.005283\n",
      "Epoch 3500, Loss: 17588749715.839333\n",
      "Epoch 3600, Loss: 17588579966.69496\n",
      "Epoch 3700, Loss: 17588474030.08081\n",
      "Epoch 3800, Loss: 17588304239.32623\n",
      "Epoch 3900, Loss: 17588225025.571407\n",
      "Epoch 4000, Loss: 17588119957.754284\n",
      "Epoch 4100, Loss: 17588079883.43334\n",
      "Epoch 4200, Loss: 17588000084.422066\n",
      "Epoch 4300, Loss: 17587954262.164223\n",
      "Epoch 4400, Loss: 17587908151.567394\n",
      "Epoch 4500, Loss: 17587857538.98192\n",
      "Epoch 4600, Loss: 17587824965.858524\n",
      "Epoch 4700, Loss: 17587841505.75064\n",
      "Epoch 4800, Loss: 17587829377.724766\n",
      "Epoch 4900, Loss: 17587843027.15782\n",
      "Epoch 5000, Loss: 17587826751.815594\n",
      "Epoch 5100, Loss: 17587782303.738804\n",
      "Epoch 5200, Loss: 17587814284.98247\n",
      "Epoch 5300, Loss: 17587794409.63005\n",
      "Epoch 5400, Loss: 17587768102.60983\n",
      "Epoch 5500, Loss: 17587805423.04757\n",
      "Epoch 5600, Loss: 17587750091.756084\n",
      "Epoch 5700, Loss: 17587791220.937027\n",
      "Epoch 5800, Loss: 17587729897.044094\n",
      "Epoch 5900, Loss: 17587794177.34648\n",
      "Epoch 6000, Loss: 17587758430.97274\n",
      "Epoch 6100, Loss: 17587724100.155376\n",
      "Epoch 6200, Loss: 17587783250.602\n",
      "Epoch 6300, Loss: 17587744181.316048\n",
      "Epoch 6400, Loss: 17587749779.474743\n",
      "Epoch 6500, Loss: 17587733910.924892\n",
      "Epoch 6600, Loss: 17587749484.0104\n",
      "Epoch 6700, Loss: 17587756609.266644\n",
      "Epoch 6800, Loss: 17587724948.231102\n",
      "Epoch 6900, Loss: 17587725412.392963\n",
      "Epoch 7000, Loss: 17587696970.971992\n",
      "Epoch 7100, Loss: 17587746445.03264\n",
      "Epoch 7200, Loss: 17587761435.951946\n",
      "Epoch 7300, Loss: 17587732611.98914\n",
      "Epoch 7400, Loss: 17587716475.732323\n",
      "Epoch 7500, Loss: 17587781855.58812\n",
      "Epoch 7600, Loss: 17587725604.25898\n",
      "Epoch 7700, Loss: 17587798365.550083\n",
      "Epoch 7800, Loss: 17587780223.41205\n",
      "Epoch 7900, Loss: 17587736453.224045\n",
      "Epoch 8000, Loss: 17587765547.66902\n",
      "Epoch 8100, Loss: 17587761768.6443\n",
      "Epoch 8200, Loss: 17587676168.150394\n",
      "Epoch 8300, Loss: 17587730782.33679\n",
      "Epoch 8400, Loss: 17587741295.882744\n",
      "Epoch 8500, Loss: 17587821787.82055\n",
      "Epoch 8600, Loss: 17587776527.61233\n",
      "Epoch 8700, Loss: 17587757389.50939\n",
      "Epoch 8800, Loss: 17587695668.115788\n",
      "Epoch 8900, Loss: 17587767829.75451\n",
      "Epoch 9000, Loss: 17587778706.88921\n",
      "Epoch 9100, Loss: 17587708278.60168\n",
      "Epoch 9200, Loss: 17587728288.757885\n",
      "Epoch 9300, Loss: 17587735349.87909\n",
      "Epoch 9400, Loss: 17587739401.82019\n",
      "Epoch 9500, Loss: 17587716049.956787\n",
      "Epoch 9600, Loss: 17587722977.060524\n",
      "Epoch 9700, Loss: 17587759657.296547\n",
      "Epoch 9800, Loss: 17587793802.167995\n",
      "Epoch 9900, Loss: 17587764829.18052\n"
     ]
    }
   ],
   "source": [
    "parametersModel = {\n",
    "  'poly_degree': 2,\n",
    "  'poly_alpha': 0.0001,\n",
    "  'poly_epochs': 100000,\n",
    "  'poly_lambda': 0.01,\n",
    "};\n",
    "\n",
    "NR = NonlinearRegression(poly_degree=parametersModel['poly_degree'], X=x_train_scaled)\n",
    "NR.train(x_train_scaled, y_train.to_numpy(), poly_alpha=parametersModel['poly_alpha'], poly_epochs=parametersModel['poly_epochs'], poly_lambda=parametersModel['poly_lambda']);\n",
    "\n",
    "y_pred_train = NR.predict(x_train_scaled)\n",
    "y_pred_test = NR.predict(x_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c9156d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m feature_x_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m;\n\u001b[0;32m      2\u001b[0m feature_y_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m;\n\u001b[1;32m----> 4\u001b[0m fig \u001b[38;5;241m=\u001b[39m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m8\u001b[39m))\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Gráfico para el conjunto de entrenamiento.\u001b[39;00m\n\u001b[0;32m      7\u001b[0m ax1 \u001b[38;5;241m=\u001b[39m fig\u001b[38;5;241m.\u001b[39madd_subplot(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m, projection\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m3d\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "feature_x_index = 0;\n",
    "feature_y_index = 1;\n",
    "\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Gráfico para el conjunto de entrenamiento.\n",
    "ax1 = fig.add_subplot(1, 2, 1, projection='3d')\n",
    "ax1.scatter(x_train_scaled[:, feature_x_index], x_train_scaled[:, feature_y_index], y_train, color='blue', label='Datos reales')\n",
    "ax1.scatter(x_train_scaled[:, feature_x_index], x_train_scaled[:, feature_y_index], y_pred_train, color='red', label='Predicción del modelo')\n",
    "ax1.set_title('Conjunto de Entrenamiento')\n",
    "ax1.set_xlabel('Característica x')\n",
    "ax1.set_ylabel('Característica y')\n",
    "ax1.set_zlabel('Y (MTO_PIA)')\n",
    "ax1.legend()\n",
    "\n",
    "# Gráfico para el conjunto de prueba.\n",
    "ax2 = fig.add_subplot(1, 2, 2, projection='3d')\n",
    "ax2.scatter(x_test_scaled[:, feature_x_index], x_test_scaled[:, feature_y_index], y_test, color='blue', label='Datos reales')\n",
    "ax2.scatter(x_test_scaled[:, feature_x_index], x_test_scaled[:, feature_y_index], y_pred_test, color='red', label='Predicción del modelo')\n",
    "ax2.set_title('Conjunto de Prueba')\n",
    "ax2.set_xlabel('Característica x')\n",
    "ax2.set_ylabel('Característica y')\n",
    "ax2.set_zlabel('Y (MTO_PIA)')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0dce3f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
